{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Contextual_Bandit_and_Thompson_Sampling_Ange_Valli.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIFb60gu1Nwk"
      },
      "source": [
        "See original notebook : https://drive.google.com/drive/folders/16qThuQ-I4_0Rg2LFGUkiD4PbEnmQX3UH?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "AT3dk_hdFtFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Mount Google Drive to access files"
      ],
      "metadata": {
        "id": "MoDwM3l4FxSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jbYA1a42ALNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Add a shortcut from the shared folder _Contextual_Bandit_and_Thompson_Sampling_ into your Google Drive folders"
      ],
      "metadata": {
        "id": "yKB61fDoAU1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Change current folder for the _Contextual_Bandit_and_Thompson_Sampling_ folder (don't forget to change the file path if different)"
      ],
      "metadata": {
        "id": "0cs5_qqBAm15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Contextual_Bandit_and_Thompson_Sampling"
      ],
      "metadata": {
        "id": "3Nzbe4yQA0Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxu2LChUzCSW"
      },
      "source": [
        "## Problem description\n",
        "\n",
        "#### Veepee's business description\n",
        "\n",
        "Veepee is an online flash sales website which offers a large number of new sales everyday with huge discounts up to 70%. Sales are available for a very short period of time.\n",
        "\n",
        "On Veepee's website, there are **about 200 flash sales on the home page** on a given day divided into several sectors like fashion, accessories, toys, watches, home appliances, sports equipment, technology, wines, travel, etc.\n",
        "\n",
        "New sales open every day and old sales either continue or stop.\n",
        "\n",
        "#### Homepage recommendation problem\n",
        "\n",
        "Because the number of sales (also called operations) is important, users might not scroll until the end of the homepage to see all the banners and might leave Veepee if no sales at the top of the page are relevant to them.\n",
        "\n",
        "Thus, the main goal of the homepage customization will be to rank the banners so that the most relevant active sales for a customer appear on top of the page.\n",
        "\n",
        "For that, we rely on the user's previous orders and preferences but also on sales popularity and other global information.\n",
        "\n",
        "#### First connection issue\n",
        "\n",
        "Because the ranking algorithm uses members features which are processed once a day, when a user comes for the first time, its home page is not personalized until the next day.\n",
        "\n",
        "**The goal of these notebook is to make a first ranking by presenting the user some operations and ask him if he is interested or not.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0M4qWmszCSA"
      },
      "source": [
        "# Installation\n",
        "\n",
        "- Python version 3.7.x\n",
        "- `pip install -u pip`\n",
        "- `pip install -r requirements.txt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqNhIdXcz3HB"
      },
      "source": [
        "# In Google Colab, run this cell to import used library version\n",
        "\n",
        "!pip install numpy==1.19.2\n",
        "!pip install pandas==1.1.4\n",
        "!pip install scikit-learn==0.23.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9F6PTsfzCSb"
      },
      "source": [
        "# 1 - Random propositions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YCK1nfTzCSf"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuqDarLKzCSh"
      },
      "source": [
        "import pandas as pd\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIIAl1CozCSj"
      },
      "source": [
        "random.seed(84)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECI7zC53zCSn"
      },
      "source": [
        "%%javascript\n",
        "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
        "    return false;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYU28CF_zCSu"
      },
      "source": [
        "## Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDKNBJHZzCSw"
      },
      "source": [
        "The dataframe contains the features of the operation and the related banner. For the train we took all the operations displayed on the homepage on `2020-08-04` (these operation can be new operations or ongoing ones). For the test set, we only took the operations which started in `2020-10-30`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE2gu-4VzCSx"
      },
      "source": [
        "train_ops = pd.read_pickle(\"train_2020-08-04.pickle\")\n",
        "test_ops = pd.read_pickle(\"test_2020-10-30.pickle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTm_PX_UzCSy"
      },
      "source": [
        "### Operations features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKgCl6nSzCS1"
      },
      "source": [
        "train_ops.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StAIf1PtzCS4"
      },
      "source": [
        "- `operationcd`: code of the operation\n",
        "- `secteur_principal`: first level of two of the internal taxonomy\n",
        "- `sous_secteur_principal`: second level of two of the internal taxonomy\n",
        "- `business_type`: 'Vente privée', 'Entertainment', 'MEDIA', 'Vin', 'One Day', 'Voyage'\n",
        "- `brand`: name of the brand\n",
        "- `operation_type`: 'Classique', 'VBI interne', 'Thématique', 'Vin classique', 'One Day', 'Hotel Planet', 'Séquence caviste'\n",
        "- `front_secteur`: first level of two of the homepage taxonomy\n",
        "- `front_sous_secteur`: second level of two of the homepage taxonomy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ-L-fHizCS5"
      },
      "source": [
        "### Operations banners"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqRFddJDzCS7"
      },
      "source": [
        "from IPython import display\n",
        "from base64 import b64decode\n",
        "\n",
        "display.Image(train_ops.loc[56].banner)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOPkr0pmzCS_"
      },
      "source": [
        "So the homepage consists on 200 banners like this one displayed in two columns. On the banner you will have the brand name of the operation and a specific visual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8sgCkhZzCTA"
      },
      "source": [
        "## Exercice Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBHXCslrzCTA"
      },
      "source": [
        "In the next cells we will simulate a small survey given to a new user:\n",
        "\n",
        "We will display operations randomly and ask the user for his/her interest. Each operation is selected completely randomly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdEbXGnJzCTB"
      },
      "source": [
        "# Imports\n",
        "from ipywidgets import AppLayout, Button, GridspecLayout, Image, Layout "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbiUhlDyzCTC"
      },
      "source": [
        "# Add empty column to record actions\n",
        "train_ops['interested'] = None\n",
        "test_ops['interested'] = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfjDp1cZzCTC"
      },
      "source": [
        "def create_expanded_button(description, button_style):\n",
        "    return Button(\n",
        "        description=description,\n",
        "        button_style=button_style,\n",
        "        layout=Layout(height='auto', width='auto')\n",
        "    )\n",
        "\n",
        "\n",
        "def random_row():\n",
        "    return random.randint(0, len(train_ops) - 1)\n",
        "\n",
        "\n",
        "def get_banner(index, dataset):\n",
        "    return Image(\n",
        "        value=dataset.loc[index].banner,\n",
        "        format='jpg',\n",
        "        width=300,\n",
        "        height=400,\n",
        "    )\n",
        "\n",
        "\n",
        "def plot_grid(df):\n",
        "    n_rows = (len(df)//3) + (len(df) % 3 != 0)\n",
        "    grid = GridspecLayout(n_rows , 3)\n",
        "\n",
        "    cpt = 0\n",
        "    for i in range(n_rows):\n",
        "        for j in range(3):\n",
        "            if cpt < len(df):\n",
        "                grid[i, j] = get_banner(df.index[cpt], df)\n",
        "                cpt += 1 \n",
        "    return grid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ngbfzAN5zCTD"
      },
      "source": [
        "# Setting the left button: Not Interested\n",
        "left_button = create_expanded_button('Not Interested', 'danger')\n",
        "\n",
        "# Setting the image in the center\n",
        "current_row_number = random_row()\n",
        "img = get_banner(current_row_number, train_ops)\n",
        "\n",
        "# Setting the right button: Interested\n",
        "right_button = create_expanded_button('Interested', 'success')\n",
        "\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    \"\"\"\n",
        "    Update the values related to the users choice\n",
        "    Choose a new operation to display\n",
        "    Update the banner displayed\n",
        "    \"\"\"\n",
        "    update_values(b)\n",
        "    row_number = choose_row_number()\n",
        "    update_banner(b, row_number)\n",
        "    \n",
        "\n",
        "def update_values(b):\n",
        "    \"\"\"\n",
        "    Update the dataframe column \"interested\" using user's action\n",
        "    \"\"\"\n",
        "    global current_row_number\n",
        "\n",
        "    interested = b.description == \"Interested\"\n",
        "    train_ops.loc[current_row_number, 'interested'] = interested\n",
        "    \n",
        "\n",
        "def choose_row_number():\n",
        "    \"\"\"\n",
        "    Choose randomly a new operation not already seen\n",
        "    \"\"\"\n",
        "    row_number = random_row()\n",
        "    if train_ops.loc[row_number].interested is None:\n",
        "        return row_number\n",
        "    \n",
        "    while(train_ops.loc[row_number].interested is not None):\n",
        "        row_number = random_row()\n",
        "    \n",
        "    return row_number\n",
        "\n",
        "\n",
        "def update_banner(b, row_number):\n",
        "    \"\"\"\n",
        "    Update the value of the image widget with the new banner's string\n",
        "    \"\"\"\n",
        "    global current_row_number\n",
        "\n",
        "    current_row_number = row_number\n",
        "    banner = train_ops.loc[current_row_number].banner\n",
        "    img.value = banner\n",
        "\n",
        "# Set the on_click function to the button\n",
        "left_button.on_click(on_button_clicked)\n",
        "right_button.on_click(on_button_clicked)\n",
        "\n",
        "# https://ipywidgets.readthedocs.io/en/stable/examples/Layout%20Templates.html#AppLayout\n",
        "AppLayout(\n",
        "    left_sidebar=left_button,\n",
        "    center=img,\n",
        "    right_sidebar=right_button\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcbg1ojbzCTE"
      },
      "source": [
        "### Sum up operations seen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmMukXvdzCTF"
      },
      "source": [
        "#### Liked"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqJq4FwszCTF"
      },
      "source": [
        "viewed_ops = train_ops[~train_ops['interested'].isna()]\n",
        "plot_grid(viewed_ops[viewed_ops.interested])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uT5pUnRzCTK"
      },
      "source": [
        "#### Not Liked"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tA_W4HhzCTL"
      },
      "source": [
        "plot_grid(viewed_ops[~viewed_ops.interested.astype(bool)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyYvlUbGzCTO"
      },
      "source": [
        "## Learn and Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRu3I4p3zCTQ"
      },
      "source": [
        "### Logistic Regression\n",
        "\n",
        "Let us first implement a simple logistic regression and learn the user's preferences.\n",
        "\n",
        "We will then be able to rank the test operations based on the previous feedbacks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcQ71yyrzCTR"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgkIIZKNzCTR"
      },
      "source": [
        "###############################################################################\n",
        "# - Train simple Logistic Regression model on the features and feedbacks      #\n",
        "# - Make predictions on the test set                                          #\n",
        "# - Add a new column to the test set with the predicted values                #\n",
        "###############################################################################\n",
        "\n",
        "## Prepare the data\n",
        "\n",
        "feature_columns = ['secteur_principal', # Interesting columns\n",
        "    'sous_secteur_principal', 'business_type', 'brand',\n",
        "    'operation_type', 'front_secteur', 'front_sous_secteur']\n",
        "all_data = pd.concat([train_ops, test_ops], ignore_index=True)\n",
        "all_data = all_data.loc[:, feature_columns] # Select only columns of features\n",
        "encoded_data = pd.get_dummies(all_data)  # Encode data\n",
        "\n",
        "# Retrieve train and test set\n",
        "train_encoded = encoded_data.iloc[viewed_ops.index,:]  # Only viewed ones\n",
        "test_encoded = encoded_data.iloc[len(train_ops):,:]  # All test set\n",
        "\n",
        "# Build binary labels for training set (0 for False and 1 for True)\n",
        "train_label = [int(x) for x in viewed_ops.interested]\n",
        "\n",
        "# Train the model\n",
        "model = LogisticRegression(random_state=0).fit(train_encoded, train_label)\n",
        "\n",
        "# Predict on the test set\n",
        "prediction = model.predict_proba(test_encoded)[:,1]\n",
        "test_ops['prediction'] = prediction\n",
        "print(sorted(prediction, reverse=True))\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "sorted_ops = test_ops.sort_values(by=\"prediction\", ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wih5b78zCTS"
      },
      "source": [
        "#### Display ranking on test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL8A5-X3zCTS"
      },
      "source": [
        "plot_grid(sorted_ops)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYQ_hQRVzCTT"
      },
      "source": [
        "### Conclusions\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "Let us act as a specific customer who is interested by food, drinks, sport, and any kind of travel. All corresponding offers are validated, the others are discarded.\n",
        "\n",
        "We follow the previous guidelines for the first 20 offers (See Appendix to reproduce the experiment). Our use case leads to validate 8 of them. After analyzing this data, we can predict the interest on the test set: 4 offers among the 44 available have a score higher than 50% and would actually be accepted. It clearly fits the needs of our test customer, favoring Trips and sports (The first proposition \"Isola 2000\" is both), and then food. Offers number 8 to 12 have the same score of 34% and fits the interest of food-related products.\n",
        "\n",
        "As for Google requests, these first results are the most important: we do not care a lot about ordering less interesting examples. But we can notice that all satisfying offers were correctly picked; they can be found in the 9 first offers.\n",
        "\n",
        "We saw that the results of our example are convincing. This approach ensures that we collect data on a wide range of products because they are given randomly. On the other hand, the default of this is that the customer has to see lots of potentially uninteresting offers. In this example, the 6 last offers were not considered as interesting, whereas we should expect improvements for a really efficient website.\n",
        "\n",
        "To be more realistic, it is hard to imagine that a online customer would spend too much time parsing random offers. Let us redo the experience with fewer data. With the 5 first offers, we already can correctly predict 4 interesting offers from our test set. Actually, even very few data can extract interesting information. However, it is an incomplete view of our needs: the interest for food has not been spotted with this small amount of data.\n",
        "\n",
        "###############################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xvjr_2eL4bKD"
      },
      "source": [
        "### Appendix\n",
        "\n",
        "To reproduce the experiment with 20 offers, set the random seed to 84. Following offers were accepted (in chronological order): Afrique, Pop Bottles, Sport-Elec, Carambar, Montagne Été, FitFiu, Sur les routes de France, Les Landes. And the other were not interesting.\n",
        "\n",
        "For the 5-offer test, reset the random seed to 84 and validate only Afrique, Pop Bottles and Sport-Elec. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpCNTkgmzCTV"
      },
      "source": [
        "%reset -f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NBfUxzHzCTV"
      },
      "source": [
        "# 2 - Online Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbXxD5RdzCTW"
      },
      "source": [
        "## The Contextual Bandit\n",
        "\n",
        "The Contextual Bandit is just like the Multi-Armed bandit problem but now the reward probability distribution depends on external variables. Therefore, we add the notion of **context** or **state** to support our decision.\n",
        "\n",
        "We're going to suppose that the probabilty of reward is of the form\n",
        "\n",
        "$$\\theta(x) = \\frac{1}{1 + exp(-f(x))}$$\n",
        "\n",
        "where \n",
        "\n",
        "$$f(x) = \\beta_0 + \\sum_{i=0}^{d}{\\beta_i \\cdot x_i} + \\epsilon$$\n",
        "\n",
        "which is just assuming that the probability of reward linearly depends of an external variable $x$ with logistic link."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-dMSmPXzCTW"
      },
      "source": [
        "- $x$: the context. Features of the operation.\n",
        "- $d$: size of the context\n",
        "- $\\beta_i$: the param learned to predict the probability of interest\n",
        "- $\\theta(x)$: The logistic normalization to compute the probability of reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsLMmZBJzCTX"
      },
      "source": [
        "### Logistic Regression\n",
        "\n",
        "Let us implement a regular logistic regression, and use an $\\epsilon$-greedy policy to choose which bandit to activate. We try to learn the logistic function behind each bandit:\n",
        "\n",
        "$$\\theta(x) = \\frac{1}{1 + exp(-f(x))}$$\n",
        "\n",
        "where \n",
        "\n",
        "$$f(x) = \\beta_0 + \\sum_{i=0}^{d}{\\beta_i \\cdot x_i} + \\epsilon$$\n",
        "\n",
        "And select the operation which maximizes $\\theta(x)$, except when, with $\\epsilon$ probability, we select a random action (excluding the greedy action)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3Xz2Y6HzCTY"
      },
      "source": [
        "import numpy as np  # added\n",
        "import pandas as pd\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxR5hSA9zCTZ"
      },
      "source": [
        "random.seed(84)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abJUwDllzCTa"
      },
      "source": [
        "%%javascript\n",
        "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
        "    return false;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkG6RdM8zCTd"
      },
      "source": [
        "train_ops = pd.read_pickle(\"train_2020-08-04.pickle\")\n",
        "test_ops = pd.read_pickle(\"test_2020-10-30.pickle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LmPsEmZzCTg"
      },
      "source": [
        "train_ops['interested'] = None\n",
        "test_ops['interested'] = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTCei1akzCTh"
      },
      "source": [
        "from ipywidgets import AppLayout, Button, GridspecLayout, Image, Layout "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjAO-QsazCTh"
      },
      "source": [
        "def create_expanded_button(description, button_style):\n",
        "    return Button(\n",
        "        description=description,\n",
        "        button_style=button_style,\n",
        "        layout=Layout(height='auto', width='auto')\n",
        "    )\n",
        "\n",
        "def random_row():\n",
        "    return random.randint(0, len(train_ops) - 1)\n",
        "\n",
        "def get_banner(index, dataset):\n",
        "    return Image(\n",
        "        value=dataset.loc[index].banner,\n",
        "        format='jpg',\n",
        "        width=300,\n",
        "        height=400,\n",
        "    )\n",
        "\n",
        "def plot_grid(df):\n",
        "    n_rows = (len(df)//3) + (len(df) % 3 != 0)\n",
        "    grid = GridspecLayout(n_rows , 3)\n",
        "\n",
        "    cpt = 0\n",
        "    for i in range(n_rows):\n",
        "        for j in range(3):\n",
        "            if cpt < len(df):\n",
        "                grid[i, j] = get_banner(df.index[cpt], df)\n",
        "                cpt += 1 \n",
        "    return grid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3pEnfHxzCTi"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "35gSdvinzCTj"
      },
      "source": [
        "left_button = create_expanded_button('Not Interested', 'danger')\n",
        "\n",
        "current_row_number = random_row()\n",
        "img = get_banner(current_row_number, train_ops)\n",
        "\n",
        "right_button = create_expanded_button('Interested', 'success')\n",
        "\n",
        "EPSILON = 0.1\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    update_values(b)\n",
        "    row_number = choose_row_number()\n",
        "    update_banner(b, row_number)\n",
        "\n",
        "    \n",
        "def update_values(b):\n",
        "    global current_row_number\n",
        "    \n",
        "    interested = b.description == \"Interested\"\n",
        "    train_ops.loc[current_row_number, 'interested'] = interested\n",
        "\n",
        "\n",
        "def update_banner(b, row_number):\n",
        "    global current_row_number\n",
        "\n",
        "    current_row_number = row_number\n",
        "    banner = train_ops.loc[current_row_number].banner\n",
        "    img.value = banner\n",
        "\n",
        "ready_to_train = False\n",
        "def choose_row_number(epsilon=EPSILON):\n",
        "    \"\"\"Choose a sample number to display\n",
        "\n",
        "    - Make a random prediction when\n",
        "      - the training is not possible\n",
        "      - a random value is below epsilon\n",
        "    - Else display the most probable operation based on a trained model\n",
        "      (using the previous algorithm)\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    epsilon: float\n",
        "        proportion of exploration - instead of exploitation\n",
        "\n",
        "    \"\"\"\n",
        "    # Check if train is possible\n",
        "    # (If there is at least one interesting and one not interesting)\n",
        "    viewed_ops = train_ops[~train_ops['interested'].isna()]\n",
        "    nb_viewed = len(viewed_ops)\n",
        "    nb_true = len(viewed_ops[viewed_ops.interested])\n",
        "    nb_false = nb_viewed - nb_true\n",
        "    ready_to_train = nb_true and nb_false\n",
        "    \n",
        "    if not ready_to_train or random.random() < epsilon:\n",
        "        ## Explore\n",
        "        row_number = random_row()\n",
        "        # Ensure that the row has not be seen so far\n",
        "        while(train_ops.loc[row_number].interested is not None):\n",
        "            row_number = random_row()\n",
        "        return row_number\n",
        "    else:\n",
        "        # Exploit\n",
        "        ## Train a new model\n",
        "        feature_columns = ['secteur_principal', # Interesting columns\n",
        "            'sous_secteur_principal', 'business_type', 'brand',\n",
        "            'operation_type', 'front_secteur', 'front_sous_secteur']\n",
        "        # Encode data\n",
        "        all_encoded_data = pd.get_dummies(train_ops.loc[:, feature_columns])\n",
        "        # Select only viewed ones for training\n",
        "        train_encoded = all_encoded_data.iloc[viewed_ops.index,:]\n",
        "        # Build binary labels for training set (0 for False and 1 for True)\n",
        "        train_label = [int(x) for x in viewed_ops.interested]\n",
        "\n",
        "        model = LogisticRegression(random_state=0).fit(train_encoded, train_label)\n",
        "\n",
        "        # Select the best row (no seen yet)\n",
        "        prediction = model.predict_proba(all_encoded_data)[:,1]\n",
        "        best_row = np.argmax(prediction)\n",
        "        while best_row in viewed_ops.index:\n",
        "            prediction[best_row] = 0\n",
        "            best_row = np.argmax(prediction)\n",
        "\n",
        "    return best_rows\n",
        "\n",
        "left_button.on_click(on_button_clicked)\n",
        "right_button.on_click(on_button_clicked)\n",
        "\n",
        "AppLayout(\n",
        "    left_sidebar=left_button,\n",
        "    center=img,\n",
        "    right_sidebar=right_button\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8QY4JZwzCTk"
      },
      "source": [
        "### Sum up operations seen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxTPLa9-zCTl"
      },
      "source": [
        "#### Liked"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-UMoKAPzCTn"
      },
      "source": [
        "viewed_ops = train_ops[~train_ops['interested'].isna()]\n",
        "plot_grid(viewed_ops[viewed_ops.interested])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhfmEpt-zCTo"
      },
      "source": [
        "#### Not Liked"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BLvWCzSzCTp"
      },
      "source": [
        "plot_grid(viewed_ops[~viewed_ops.interested.astype(bool)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi_1MV_zzCTu"
      },
      "source": [
        "### Redo Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdRLbAQKzCTw"
      },
      "source": [
        "###############################################################################\n",
        "# - Train simple Logistic Regression model on the features and feedbacks      #\n",
        "# - Make predictions on the test set                                          #\n",
        "# - Add a new column to the test set with the predicted values                #\n",
        "###############################################################################\n",
        "\n",
        "## Prepare the data\n",
        "feature_columns = ['secteur_principal', # Interesting columns\n",
        "    'sous_secteur_principal', 'business_type', 'brand',\n",
        "    'operation_type', 'front_secteur', 'front_sous_secteur']\n",
        "all_data = pd.concat([train_ops, test_ops], ignore_index=True)\n",
        "all_data = all_data.loc[:, feature_columns]  # Select only columns of features\n",
        "encoded_data = pd.get_dummies(all_data)  # Encode data\n",
        "\n",
        "# Retrieve train and test set\n",
        "train_encoded = encoded_data.iloc[viewed_ops.index,:]  # Only viewed ones\n",
        "test_encoded = encoded_data.iloc[len(train_ops):,:]  # All test set\n",
        "\n",
        "# Build binary labels for training set (0 for False and 1 for True)\n",
        "train_label = [int(x) for x in viewed_ops.interested]\n",
        "\n",
        "# Train the model\n",
        "model = LogisticRegression(random_state=0).fit(train_encoded, train_label)\n",
        "\n",
        "# Predict on the test set\n",
        "prediction = model.predict_proba(test_encoded)[:,1]\n",
        "test_ops['prediction'] = prediction\n",
        "print(sorted(prediction, reverse=True))\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "sorted_ops = test_ops.sort_values(by=\"prediction\", ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW3TkAOTzCTx"
      },
      "source": [
        "plot_grid(sorted_ops)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39sPdayYzCTy"
      },
      "source": [
        "### Conclusions\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "The previous experiment is redone (20 offers, validating food-sport-trips). We obtain 18 interesting offers, which is a lot more than the previous algorithm. However, they only deal with trips. Once an interesting sector have been detected, it really focuses on it - it is a good thing , but there is a lack of exploration, here. The two first predictions are trips: they are very highly rated. We could also understand the link with the third one \"Verisure\": if we go in a trip, we may need an alarm. But then, there was no focus on sport or food.\n",
        "\n",
        "Hence, it is also important to collect some negative examples. A client can tolerate such offers if he still sees some that are attractive. Furthermore, it tends to highlight more interesting ones. One way of achieving this is to increase the `EPSILON` parameter: it ensures more exploration, more diversity.\n",
        "\n",
        "A pathological example can be found by extending the tastes of the test customer to all kitchen-related offers. If we stick to these guidelines, the second offer \"Moulinex Rowenta\" is accepted; but instead of encouraging other kitchen-related objects, it repeatedly favors vacuum cleaners and electronics, which are not what we want. It quickly focuses on the previous offers... but it could go totally wrong. A complementary explaination for this setback is that the interest to \"kitchen-related\" objects does not perfectly fit the features described in our data: this category is far less obvious than \"Trips\" or \"Clothes\" for example.\n",
        "\n",
        "In a nutshell, this method is efficient for short-term recommandations for it reacts efficiently to interesting offers. However, it needs to fine-tune the trade-off between exploitation (taking advantage of the data) and exploration (diversification) in order to avoid being stuck in a too precise domain.\n",
        "\n",
        "###############################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97-PewugOPKx"
      },
      "source": [
        "# Appendix\n",
        "\n",
        "To reproduce the experiment with 20 offers, set again the random seed to 84. Only the two first offers are refused: \"Sunny Life\" and \"Moulinex Rowenta\". All the following are interesting for our customer (related to trips)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf3wwyvZzCTz"
      },
      "source": [
        "%reset -f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZWx50YyzCTz"
      },
      "source": [
        "### Thompson Sampling\n",
        "\n",
        "In 2011, Chapelle & Li published the paper \"[An Empirical Evaluation of Thompson Sampling](https://papers.nips.cc/paper/4321-an-empirical-evaluation-of-thompson-sampling.pdf)\" that helped revive the interest on Thompson Sampling, showing favorable empirical results in comparison to other heuristics. We're going to borrow the Online Logistic Regression algorithm (Algorithm 3) from the paper. Basically, it's a bayesian logistic regression where we define a prior distribution for our weights $\\beta_i$, instead of learning just a single value for them (the expectation of the distribution). \n",
        "\n",
        "So, our model, just like the greedy algorithm, is:\n",
        "\n",
        "$$\\theta = \\frac{1}{1 + exp(-f(x))}$$\n",
        "\n",
        "where \n",
        "\n",
        "$$f(x) = \\beta_0 + \\sum_{i=0}^{d}{\\beta_i \\cdot x_i} + \\epsilon$$\n",
        "\n",
        "but the weights are actually assumed to be distributed as independent gaussians:\n",
        "\n",
        "$$\\beta_i = \\mathcal{N}(m_i,q_i^{-1})$$\n",
        "\n",
        "We initialize all $q_i$'s with a hyperparamenter $\\lambda$, which is equivalent to the $\\lambda$ used in L2 regularization. Then, at each new training example (or batch of examples) we make the following calculations:\n",
        "\n",
        "1. Find $\\textbf{w}$ as the minimizer of $\\frac{1}{2}\\sum_{i=1}^{d} q_i(w_i - m_i)^2 + \\sum_{j=1}^{n} \\textrm{log}(1 + \\textrm{exp}(-y_jw^Tx_j))$\n",
        "2. Update $m_i = w_i$ and perform $q_i = q_i + \\sum_{j=1}^{n} x^2_{ij}p_j(1-p_j)$ where $p_j = (1 + \\textrm{exp}( -w^Tx_j))^{-1}$ ([Laplace approximation](https://en.wikipedia.org/wiki/Laplace%27s_method))\n",
        "\n",
        "In essence, we basically altered the logistic regression fitting process to accomodate distributions for the weights. Our Normal priors on the weights are iteratively updated and as the number of observations grow, our uncertainty over their means is reduced. \n",
        "\n",
        "We can also increase incentives for exploration or exploitation by defining a hyperparameter $\\alpha$, which multiplies the variance of the Normal priors:\n",
        "\n",
        "$$\\beta_i = \\mathcal{N}(m_i,\\alpha \\cdot{} q_i^{-1})$$\n",
        "\n",
        "With $0 < \\alpha < 1$ we reduce the variance of the Normal priors, inducing the algorithm to be greedier, whereas with $\\alpha > 1$ we prioritize exploration. Let us implement the algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bihJVawXzCT0"
      },
      "source": [
        "- $x$: the context. Features of the operation.\n",
        "- $\\beta_i$: the param learned to predict the probability of interest\n",
        "- $\\theta(x)$: The logistic normalization to compute the probability of reward\n",
        "\n",
        "\n",
        "- $w$: weights vector\n",
        "- $m$ and $q$: parameters of the normal priors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15lrON5bzCT0"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrtt9eaQzCT0"
      },
      "source": [
        "random.seed(84)\n",
        "np.random.seed(7)  # Added"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-PbureVzCT1"
      },
      "source": [
        "%%javascript\n",
        "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
        "    return false;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieNAp5FszCT2"
      },
      "source": [
        "train_ops = pd.read_pickle(\"train_2020-08-04.pickle\")\n",
        "test_ops = pd.read_pickle(\"test_2020-10-30.pickle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzBfKX9azCT2"
      },
      "source": [
        "train_ops['interested'] = None\n",
        "test_ops['interested'] = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz3lUGj-zCT3"
      },
      "source": [
        "from ipywidgets import AppLayout, Button, GridspecLayout, Image, Layout "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGi_pghMzCT3"
      },
      "source": [
        "def create_expanded_button(description, button_style):\n",
        "    return Button(\n",
        "        description=description,\n",
        "        button_style=button_style,\n",
        "        layout=Layout(height='auto', width='auto')\n",
        "    )\n",
        "\n",
        "\n",
        "def random_row():\n",
        "    return random.randint(0, len(train_ops) - 1)\n",
        "\n",
        "\n",
        "def get_banner(index, dataset):\n",
        "    return Image(\n",
        "        value=dataset.loc[index].banner,\n",
        "        format='jpg',\n",
        "        width=300,\n",
        "        height=400,\n",
        "    )\n",
        "\n",
        "\n",
        "def plot_grid(df):\n",
        "    n_rows = (len(df)//3) + (len(df) % 3 != 0)\n",
        "    grid = GridspecLayout(n_rows , 3)\n",
        "\n",
        "    cpt = 0\n",
        "    for i in range(n_rows):\n",
        "        for j in range(3):\n",
        "            if cpt < len(df):\n",
        "                grid[i, j] = get_banner(df.index[cpt], df)\n",
        "                cpt += 1 \n",
        "    return grid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9tud_N2zCT6"
      },
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "class OnlineLogisticRegression:\n",
        "\n",
        "    # initializing\n",
        "    def __init__(self, n_dim, lambda_=5, alpha=5.0):\n",
        "\n",
        "        # the only hyperparameter is the deviation on the prior (L2 regularizer)\n",
        "        self.lambda_ = lambda_; self.alpha = alpha\n",
        "\n",
        "        # initializing parameters of the model\n",
        "        self.n_dim = n_dim,\n",
        "        self.m = np.zeros(self.n_dim)\n",
        "        self.q = np.ones(self.n_dim) * self.lambda_\n",
        "\n",
        "        # initializing weights\n",
        "        self.w = np.random.normal(\n",
        "            self.m,\n",
        "            self.alpha * (self.q)**(-1.0),\n",
        "            size = self.n_dim\n",
        "        )\n",
        "\n",
        "    # the loss function\n",
        "    def loss(self, w, *args):\n",
        "        X, y = args\n",
        "\n",
        "        #######################################################################\n",
        "        # Implement the computation of w                                      #\n",
        "        #######################################################################\n",
        "\n",
        "        # Loss to minimize\n",
        "        loss = 0.5 * (self.q * (self.m - w)**2).sum(axis=0) + np.array([\n",
        "                np.log(1. + np.exp((-1) * y[j] * w.dot(X[j])))\n",
        "                for j in range(y.shape[0])\n",
        "            ]).sum(axis=0)\n",
        "\n",
        "        #######################################################################\n",
        "        return loss\n",
        "\n",
        "    # the gradient\n",
        "    def grad(self, w, *args):\n",
        "        X, y = args\n",
        "\n",
        "        second_calculus = (-1) * np.array([\n",
        "            y[j] *  X[j] / (1. + np.exp(y[j] * w.dot(X[j])))\n",
        "            for j in range(y.shape[0])\n",
        "        ]).sum(axis=0)\n",
        "\n",
        "        w = self.q * (w - self.m) + second_calculus\n",
        "        \n",
        "        return w\n",
        "\n",
        "    # method for sampling weights\n",
        "    def get_weights(self):\n",
        "        #######################################################################\n",
        "        # Implement the computation beta_i                                    #\n",
        "        #######################################################################\n",
        "        \n",
        "        weights = np.random.normal(\n",
        "            self.m,\n",
        "            self.alpha * (self.q)**(-1.0),\n",
        "            size=self.n_dim\n",
        "        )\n",
        "        \n",
        "        #######################################################################\n",
        "        \n",
        "        return weights\n",
        "\n",
        "    # fitting method\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        # step 1, find w\n",
        "        self.w = minimize(\n",
        "            self.loss,\n",
        "            self.w,\n",
        "            args=(X, y),\n",
        "            jac=self.grad,\n",
        "            method=\"L-BFGS-B\",\n",
        "            options={'maxiter': 20, 'disp':True}\n",
        "        ).x\n",
        "        self.m = self.w\n",
        "\n",
        "        # step 2, update q\n",
        "        #######################################################################\n",
        "        # Update the value of q based on the computation of the p_i\n",
        "        #######################################################################\n",
        "\n",
        "        p = np.array([\n",
        "            1. + np.exp(-self.w.dot(X[j]))\n",
        "            for j in range(y.shape[0])\n",
        "        ])**(-1)\n",
        "\n",
        "        self.q += np.array([\n",
        "            np.array([\n",
        "                (X[j][i]**2) * p[j]*(1 - p[j])\n",
        "                for j in range(p.shape[0])\n",
        "            ]).sum(axis=0)\n",
        "            for i in range(self.q.shape[0])\n",
        "        ])\n",
        "        \n",
        "        #######################################################################\n",
        "\n",
        "    # probability output method, using weights sample\n",
        "    def predict_proba(self, X, mode='sample'):\n",
        "\n",
        "        # adding intercept to X\n",
        "        #X = add_constant(X)\n",
        "\n",
        "        # sampling weights after update\n",
        "        self.w = self.get_weights()\n",
        "\n",
        "        # using weight depending on mode\n",
        "        if mode == 'sample':\n",
        "            w = self.w # weights are samples of posteriors\n",
        "        elif mode == 'expected':\n",
        "            w = self.m # weights are expected values of posteriors\n",
        "        else:\n",
        "            raise Exception('mode not recognized!')\n",
        "\n",
        "        # calculating probabilities\n",
        "        proba = 1 / (1 + np.exp(-1 * X.dot(w)))\n",
        "        return np.array([1-proba , proba]).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6g7qToGszCT6"
      },
      "source": [
        "left_button = create_expanded_button('Not Interested', 'danger')\n",
        "\n",
        "current_row_number = random_row()\n",
        "img = get_banner(current_row_number, train_ops)\n",
        "\n",
        "right_button = create_expanded_button('Interested', 'success')\n",
        "\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    update_values(b)\n",
        "    row_number = choose_row_number()\n",
        "    update_banner(b, row_number)\n",
        "\n",
        "\n",
        "def update_values(b):\n",
        "    global current_row_number\n",
        "    \n",
        "    interested = b.description == \"Interested\"\n",
        "    train_ops.loc[current_row_number, 'interested'] = interested\n",
        "\n",
        "\n",
        "def update_banner(b, row_number):\n",
        "    global current_row_number\n",
        "\n",
        "    current_row_number = row_number\n",
        "    banner = train_ops.loc[current_row_number].banner\n",
        "    img.value = banner\n",
        "    \n",
        "\n",
        "def choose_row_number():\n",
        "    # Create the datasets\n",
        "    X = pd.get_dummies(train_ops[train_ops.columns.difference(['operationcd', 'banner', 'interested'])])\n",
        "    y = train_ops.interested\n",
        "    \n",
        "    # Fit the Model\n",
        "    olr = OnlineLogisticRegression(\n",
        "        n_dim=X.shape[1]\n",
        "    )\n",
        "    \n",
        "    olr.fit(\n",
        "        X[~y.isna()].values,\n",
        "        y.dropna().values\n",
        "    )\n",
        "\n",
        "    X.loc[y.isna(), 'prediction'] = X[y.isna()].apply(olr.predict_proba, axis=1).apply(pd.Series)[1].values\n",
        "\n",
        "    # Choose the row\n",
        "    best_row = X.sort_values(by=\"prediction\", ascending=False).index[0]\n",
        "    return best_row\n",
        "\n",
        "    \n",
        "left_button.on_click(on_button_clicked)\n",
        "right_button.on_click(on_button_clicked)\n",
        "\n",
        "AppLayout(\n",
        "    left_sidebar=left_button,\n",
        "    center=img,\n",
        "    right_sidebar=right_button\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zeg3NnS1zCT9"
      },
      "source": [
        "### Sum up operations seen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6B4487OzCT_"
      },
      "source": [
        "#### Liked"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dItlcrKhzCUB"
      },
      "source": [
        "viewed_ops = train_ops[~train_ops['interested'].isna()]\n",
        "plot_grid(viewed_ops[viewed_ops.interested])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBCxx7xdzCUC"
      },
      "source": [
        "#### Not Liked"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6M3_LcYzCUD"
      },
      "source": [
        "plot_grid(viewed_ops[~viewed_ops.interested.astype(bool)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc6M-kkXzCUD"
      },
      "source": [
        "### Redo Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xid6T2oczCUE"
      },
      "source": [
        "###############################################################################\n",
        "# - Train the Online Logistic Regression model on the features and feedbacks  #\n",
        "# - Make predictions on the test set                                          #\n",
        "# - Add a new column to the test set with the predicted values                #\n",
        "###############################################################################\n",
        "\n",
        "## Prepare the data\n",
        "feature_columns = ['secteur_principal', # Interesting columns\n",
        "    'sous_secteur_principal', 'business_type', 'brand',\n",
        "    'operation_type', 'front_secteur', 'front_sous_secteur']\n",
        "all_data = pd.concat([train_ops, test_ops], ignore_index=True)\n",
        "all_data = all_data.loc[:, feature_columns] # Select only columns of features\n",
        "encoded_data = pd.get_dummies(all_data)  # Encode data\n",
        "\n",
        "# Retrieve train and test set\n",
        "# Note: OnlineLogisticRegression fits nd.array and not pandas.DataFrame !\n",
        "train_encoded = np.array(encoded_data.iloc[viewed_ops.index,:])  # Only viewed ones\n",
        "test_encoded = np.array(encoded_data.iloc[len(train_ops):,:])  # All test set\n",
        "\n",
        "print(type(train_encoded))\n",
        "\n",
        "# Build binary labels for training set (0 for False and 1 for True)\n",
        "train_label = np.array([int(x) for x in viewed_ops.interested])\n",
        "\n",
        "# Fit the Model\n",
        "olr = OnlineLogisticRegression(n_dim=train_encoded.shape[1])\n",
        "olr.fit(train_encoded, train_label)\n",
        "\n",
        "# Predict on the test set\n",
        "prediction = olr.predict_proba(test_encoded)[:,1]\n",
        "test_ops['prediction'] = prediction\n",
        "print(sorted(prediction, reverse=True))\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "sorted_ops = test_ops.sort_values(by=\"prediction\", ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL_vxL4ozCUF"
      },
      "source": [
        "plot_grid(sorted_ops)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WVH04IlzCUF"
      },
      "source": [
        "### Conclusions\n",
        "\n",
        "In these notebooks, we implemented the Contextual Bandit problem and presented two algorithms to solve it. The first, $\\epsilon$-greedy, uses a regular logistic regression to get its greedy estimates about the expeceted rewards $\\theta(x)$. The second, Thompson Sampling, relies on the Online Logistic Regression to learn an independent normal distribution for each of the linear model weights $\\beta_i \\sim \\mathcal{N}(m_i, q_i ^ -1)$. We draw samples from these Normal posteriors in order to achieve randomization for our bandit choices.\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "Still with the same test customer, we validate 9 offers over the 20 proposed. The repartition of the interesting offers during the training part is more homogeneously than for the eps-greedy algorithm: it alternates between different kind of offers... But on the contrary of the first random algorithm, it seems to propose globally more appropriate content as the session continues. For example, the four last offers of our experiment were positive ones.\n",
        "\n",
        "The predictions on the test samples might be different for the previous ones for it does not agglomerate all the positive offers at the top. The advantages of using Thompson sampling might not be obvious with this example for the algorithm is highly stochastic. But globally, iterate the tests over more examples should better illustrate the efficiency of the randomization.\n",
        "\n",
        "Here, it ensures we test both interesting and uninteresting offers, but keeping an eye on the customer's tastes.\n",
        "\n",
        "Once again, the parameters of the algorithm could be fine-tuned. For example, increasing the parameter `alpha` favors exploration by impacting the variance of the distribution of the $\\beta_i$.\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "### Appendix\n",
        "\n",
        "For reproducibility, set the random seeds to 84 for `random` library and 7 for `numpy`. It leads to accept the following (in chronological order): Maison-Villa Appartement France, Sur les Routes de France, Carambar, Canard-Duchene, Montagne Été, Village Nature, Sport-Elec, la Grèce et ses îles, Espagne."
      ]
    }
  ]
}